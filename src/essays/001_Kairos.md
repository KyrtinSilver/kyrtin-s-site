---
title: "001 - Kairos"
layout: essay.njk
date: "2023-07-01"
---

Combating misinformation, reducing bias, and most of the other stated goals of many Responsible AI efforts require the technology we've just demonstrated, the ability to detect cognitive biases automatically, and more accurately than the average human.

Even taking the approach of minimum adjustment, where a company adapts to the minimum viable degree, we can take the example of 2-step moderation processes for many online platforms. Currently:

- These platforms typically use trigger words and other extremely simple filters to search for content that may violate their terms and interests. (Chatbot systems also do this)

- Those items that are flagged by simple filters are forwarded to facilities full of human moderators, whose working conditions are often the subject of scathing news reports.

Instead, by applying cognitive bias detection technology, they soon could:

- Have systems automatically detecting complex and ever-evolving patterns of abuse. By detecting hostile patterns of cognitive biases rather than using simple filters, these systems could detect social engineering threats and viral misinformation as well as more ordinary bad actors and abusers far more quickly and accurately.

- This material could then be sent on to human moderators. However, following an initial adjustment phase to the platform, only a subset adhering to each observed pattern could require human review.

- The reduction in human moderation required for deeply negative material could be balanced through a 50/50 split, having moderators assign positive scores and potential incentives to the least biased and most accurate content on each platform, reducing toxicity for both the platform and moderator working conditions.

A divide may now emerge between those organizations, institutions, companies, and governments who are sincere in their interest to reduce bias and those who are not.

Up until now, many of these entities have been attempting to "cut glass with a hammer", because that was the best tool they knew to be available to them. As news of this research spreads, that will quickly cease to be the case.

I'll be posting a series of specific use cases over the coming days to drive this point home, of which this is the first.

One can hope that if the umpteenth chatbot to be deployed this year, with no benchmarks to show for it, is rapidly funded to the tune of $1.3bn, that some competent investors may emerge from the current AI mess to fund technology that is actually worth deploying. Only time will tell.