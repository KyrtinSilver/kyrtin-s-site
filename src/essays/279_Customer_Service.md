---
title: "279 - Customer Service"
layout: essay.njk
date: "2025-01-24"
---

The absolute last place anyone should implement AI is in Customer Service (CS). That doesn't mean that it shouldn't be done, but rather it means that it comes in dead last in the order of implementation. This is because CS is explicitly "Damage Control", it is the place at the end of your process diagram where the rest of your systems have failed so badly that a customer is unable to resolve the problem themselves.

This lesson was learned by a few companies, who rather than attempting to outsource or automate their customer service actually worked on improving their UI/UX and other critical systems to address the vast majority of problems that rained down on their customer service prior to those fixes. If customer service is a major expense for your business, then you've massively F***ed things up across the earlier stages in your process diagram, and that is where your attention should be focused.

What remains after all reasonable fixes and UI/UX adjustments have been made are going to tend to be edge cases, the very thing that trivial AI systems like chatbots (with or without RAG or other cheap tricks duct-taped to them) are statistically going to perform the worst at. 

In fact, typically the only viable use case for such CS chatbots is as a very temporary band aide, while fixes for common UI/UX and other business problems are prepared, after which the CS chatbot ceases to serve any viable function, except to further piss off all of the people with edge cases whose time you wasted a bit more of. You could, hypothetically, create a CS chatbot that only kicks in conditionally, such as if your company has a bad update followed by a huge surge in CS activity, but that carries its own risks of further compounding the problem as it occurs. If you have a subset of predictable risks that may be countered in advance, that subset may carry an acceptable level of risk for such conditional implementation methods.

Two of the things that everyone seems to want and will tell you about are AI for coding assistance, and AI for CS, with the third that people shy away from naming being Cybersecurity. AI for coding assistance does indeed hold plenty of potential, though not in the technologies that most people are familiar with today, and even greater potential quietly resides in Cybersecurity. 

Everyone knows that their cybersecurity is crap, that the dependencies and third-parties their systems are built on are full of vulnerabilities, and any actual AI expert can tell you that LLM-based systems are vulnerable-by-design, making vulnerable every system that they touch. In psychological terms, cybersecurity is where normal coding meets sharp spikes in fear, paranoia, and shame, effectively clamping down on many discussions in a negative and self-reinforcing loop. Both communication and a lack of communication can strongly undermine efforts in this domain, leaving many paralyzed until cut. Those without viable cybersecurity today are like Schr√∂dinger's cat.

