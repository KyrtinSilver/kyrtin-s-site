---
title: "287 - Advisory Board Aspirations"
layout: essay.njk
date: "2024-12-21"
---

As my team nears our next milestone, the real-time and scalable
ICOM-based systems, along with 3 years of other feature integrations and
major upgrades, the time to appoint a fresh Advisory Board is also
approaching.

I've encouraged my team to consider people they'd like to nominate for
our new Advisory Board and brainstormed on this topic with other
colleagues, but I'd also like to offer this opportunity for collective
intelligence to my network here on LinkedIn.

We're looking for Advisory Board members who can:

- Contribute sound advice to approaching a myriad of promising
  verticals for deploying systems that deliver human-like,
  human-level, highly efficient, easily scalable, and recursively
  self-improving performance across arbitrary domains and complexity.

- Be responsible and ethical, working together to produce collective
  intelligence.

- Help make the world a better place each step of the way, bridging
  the divide between vision and reality by building that path from one
  to the other.

Note: We won't be accepting any self-nominations.

I have lined up a list of people who've weathered the AI hype very well,
candidates who I'll likely be asking and giving demonstrations of our
systems to in the coming weeks. Three of these people have already taken
the step of connecting with me on LinkedIn of their own accord, Grady
Booch, Bojan Tunguz, and Paul Burchard, with other noteworthy nominated
names including Mary Lou Jepsen, Cassie Kozyrkov, Matthew Mayo, Richard
Self, and Brian Behlendorf.

I'd also nominate Jaron Lanier if I could find a carrier pigeon able to
reach him, as his name came up both in my own thoughts and was
independently suggested by another.

As we deploy and begin testing and releasing new results of the first
minimally complete 8^th^ generation ICOM-based systems, looking to
benchmarks including SWE-bench, SimpleBench, and more, things are likely
to begin moving rather quickly. I'm also taking suggestions on
benchmarks to target, specifically focusing on those where other models
and systems score below 50%.

If OpenAI's latest joke confirms anything, it is that we're still far
ahead of everyone else, even when the other parties burn billions on
na√Øve attempts to compete. Our work was slowed down by all of the
industry's funding being redirected into dead-end technology these past
2 years, but we've continued to make progress, while they've continued
to fester.

Who would you trust to give sound advice on this matter? How would they
shape 2025?
